{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81443f2e",
   "metadata": {},
   "source": [
    "# Notebook Objective\n",
    "\n",
    "The primary objective of this notebook is to perform visual clustering of social-media images.\n",
    "For this reason, we will exclusively import and use DINO embeddings, which are specifically designed to capture visual patterns, shapes, and textures rather than semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d516fb8",
   "metadata": {},
   "source": [
    "# Test Datasets - recap\n",
    "\n",
    "We will test the search engine on two datasets:\n",
    "\n",
    "1. **Recognition of the Palestinian State (08/13 – 08/29)**  \n",
    "   This dataset was collected following France’s recognition of the State of Palestine.  \n",
    "   It includes both **texts and their associated images**, totaling **5,055 images**.\n",
    "\n",
    "2. **September 10 Demonstrations (08/20 – ~10/10)**  \n",
    "   This dataset was collected during the period surrounding the **September 10 protest in France**.  \n",
    "   It also contains **texts and their related images**, totaling **41,942 images**.\n",
    "\n",
    "Only the **image URLs** are stored — the images themselves will be **downloaded dynamically** during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac7b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "from PIL import Image  # Librairie pour ouvrir, convertir et manipuler des images locales\n",
    "from pathlib import Path \n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm  \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import warnings \n",
    "import logging  \n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "from dash import Dash, dcc, html, Input, Output\n",
    "import plotly.express as px\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"transformer\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"PIL\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()        \n",
    "path = os.getenv(\"path_folder\")\n",
    "\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67cb32",
   "metadata": {},
   "source": [
    "## Import embeddings\n",
    "\n",
    "- **`path_folder_images_embeddings`** → the **directory path** where the embeddings are saved as parquet file (to be adapted as needed).\n",
    "- **`subject`** → defines the dataset used, either `\"10_septembre\"` or `\"reconnaissance_france_palestine\"` (to be adapted).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd2b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_images_embeddings = os.path.join(path, \"img_embeddings\")\n",
    "subject_10_septembre = '10_septembre' \n",
    "subject_reconnaissance_france_palestine = 'reconnaissance_france_palestine'\n",
    "\n",
    "path_10_septembre = f\"{path_folder_images_embeddings}/{subject_10_septembre}_DINO.parquet\"\n",
    "path_reconnaissance_france_palestine = f\"{path_folder_images_embeddings}/{subject_reconnaissance_france_palestine}_DINO.parquet\"\n",
    "\n",
    "# Upload data\n",
    "df_10_septembre = pd.read_parquet(path_10_septembre)\n",
    "df_reconnaissance_france_palestine = pd.read_parquet(path_reconnaissance_france_palestine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d8021",
   "metadata": {},
   "source": [
    "## 10 septembre  \n",
    "We begin with the **“10 septembre”** dataset before moving on to the second one.  \n",
    "This approach helps avoid overlapping or duplicated code blocks that perform the same operations with only minor name changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b6269",
   "metadata": {},
   "source": [
    "### Dimensionality reduction with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b193ed53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (39839, 1280)\n",
      "CPU times: total: 3min 4s\n",
      "Wall time: 52.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_10_septembre_dino = np.vstack(df_10_septembre[\"embedding\"].values)\n",
    "print(\"Shape:\", X_10_septembre_dino.shape)  # (nb_images, 1280)\n",
    "X_10_septembre_reduced = umap.UMAP(n_components=2, random_state=42, n_neighbors=20, metric='cosine', min_dist=0, spread=1, n_jobs=-1).fit_transform(X_10_septembre_dino)\n",
    "df_10_septembre[\"x\"] = X_10_septembre_reduced[:,0]\n",
    "df_10_septembre[\"y\"] = X_10_septembre_reduced[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c864926",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ecb2986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=2\n",
      "Clusters: 1949, Bruit: 28.67%, Persistance: 0.011, Silhouette: 0.615\n",
      "i=7\n",
      "Clusters: 1289, Bruit: 24.66%, Persistance: 0.021, Silhouette: 0.631\n",
      "i=12\n",
      "Clusters: 797, Bruit: 22.67%, Persistance: 0.023, Silhouette: 0.629\n",
      "i=17\n",
      "Clusters: 565, Bruit: 21.71%, Persistance: 0.023, Silhouette: 0.602\n",
      "i=22\n",
      "Clusters: 448, Bruit: 22.31%, Persistance: 0.023, Silhouette: 0.600\n",
      "i=27\n",
      "Clusters: 366, Bruit: 22.48%, Persistance: 0.020, Silhouette: 0.593\n",
      "i=32\n",
      "Clusters: 333, Bruit: 23.52%, Persistance: 0.019, Silhouette: 0.605\n",
      "i=37\n",
      "Clusters: 288, Bruit: 24.19%, Persistance: 0.018, Silhouette: 0.594\n",
      "i=42\n",
      "Clusters: 258, Bruit: 24.65%, Persistance: 0.015, Silhouette: 0.588\n",
      "i=47\n",
      "Clusters: 231, Bruit: 25.30%, Persistance: 0.015, Silhouette: 0.584\n",
      "i=52\n",
      "Clusters: 214, Bruit: 25.52%, Persistance: 0.015, Silhouette: 0.581\n",
      "i=57\n",
      "Clusters: 198, Bruit: 27.26%, Persistance: 0.014, Silhouette: 0.587\n",
      "i=62\n",
      "Clusters: 183, Bruit: 27.99%, Persistance: 0.011, Silhouette: 0.596\n",
      "i=67\n",
      "Clusters: 170, Bruit: 29.34%, Persistance: 0.012, Silhouette: 0.601\n",
      "i=72\n",
      "Clusters: 149, Bruit: 27.36%, Persistance: 0.013, Silhouette: 0.569\n",
      "i=77\n",
      "Clusters: 141, Bruit: 28.49%, Persistance: 0.014, Silhouette: 0.584\n",
      "i=82\n",
      "Clusters: 134, Bruit: 28.39%, Persistance: 0.015, Silhouette: 0.576\n",
      "i=87\n",
      "Clusters: 121, Bruit: 28.97%, Persistance: 0.017, Silhouette: 0.571\n",
      "i=92\n",
      "Clusters: 111, Bruit: 30.49%, Persistance: 0.019, Silhouette: 0.573\n",
      "i=97\n",
      "Clusters: 101, Bruit: 31.03%, Persistance: 0.022, Silhouette: 0.567\n",
      "i=102\n",
      "Clusters: 96, Bruit: 31.53%, Persistance: 0.024, Silhouette: 0.565\n",
      "i=107\n",
      "Clusters: 94, Bruit: 31.65%, Persistance: 0.016, Silhouette: 0.564\n",
      "i=112\n",
      "Clusters: 90, Bruit: 32.74%, Persistance: 0.016, Silhouette: 0.566\n",
      "i=117\n",
      "Clusters: 86, Bruit: 33.08%, Persistance: 0.017, Silhouette: 0.562\n",
      "i=122\n",
      "Clusters: 83, Bruit: 33.39%, Persistance: 0.017, Silhouette: 0.568\n",
      "i=127\n",
      "Clusters: 80, Bruit: 33.38%, Persistance: 0.017, Silhouette: 0.569\n",
      "i=132\n",
      "Clusters: 77, Bruit: 34.36%, Persistance: 0.018, Silhouette: 0.568\n",
      "i=137\n",
      "Clusters: 75, Bruit: 34.65%, Persistance: 0.019, Silhouette: 0.565\n",
      "i=142\n",
      "Clusters: 73, Bruit: 34.99%, Persistance: 0.022, Silhouette: 0.569\n",
      "i=147\n",
      "Clusters: 70, Bruit: 35.17%, Persistance: 0.026, Silhouette: 0.569\n",
      "i=152\n",
      "Clusters: 69, Bruit: 35.16%, Persistance: 0.027, Silhouette: 0.576\n",
      "i=157\n",
      "Clusters: 65, Bruit: 35.57%, Persistance: 0.029, Silhouette: 0.567\n",
      "i=162\n",
      "Clusters: 62, Bruit: 36.77%, Persistance: 0.031, Silhouette: 0.564\n",
      "i=167\n",
      "Clusters: 54, Bruit: 31.91%, Persistance: 0.042, Silhouette: 0.534\n",
      "i=172\n",
      "Clusters: 54, Bruit: 31.91%, Persistance: 0.067, Silhouette: 0.534\n",
      "i=177\n",
      "Clusters: 44, Bruit: 26.99%, Persistance: 0.110, Silhouette: 0.444\n",
      "i=182\n",
      "Clusters: 42, Bruit: 27.29%, Persistance: 0.100, Silhouette: 0.439\n",
      "i=187\n",
      "Clusters: 41, Bruit: 27.75%, Persistance: 0.096, Silhouette: 0.442\n",
      "i=192\n",
      "Clusters: 38, Bruit: 26.35%, Persistance: 0.096, Silhouette: 0.434\n",
      "i=197\n",
      "Clusters: 37, Bruit: 26.35%, Persistance: 0.086, Silhouette: 0.433\n"
     ]
    }
   ],
   "source": [
    "test_performance(X_10_septembre_reduced, df_10_septembre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9caf28a",
   "metadata": {},
   "source": [
    "The best compremise was when min cluster size is 177 (noise 27%,a good silhouette value of 0.444 and the higher persistance 0.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140fb5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 45, Bruit: 26.986119129496224%, Persistance: 0.10968416775996738\n"
     ]
    }
   ],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=177, min_samples=5, metric=\"euclidean\")\n",
    "labels = clusterer.fit_predict(X_10_septembre_reduced)\n",
    "df_10_septembre[\"cluster\"] = (labels).astype(str)\n",
    "print(f\"Cluster: {len(df_10_septembre.cluster.unique())}, Bruit: {len(df_10_septembre[df_10_septembre.cluster == '-1'])/len(df_10_septembre)*100}%, Persistance: {(clusterer.cluster_persistence_).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded6f70",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6334f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_images_name = os.path.join(path, \"img_data\")\n",
    "image_dir_10_septembre = os.path.join(path_folder_images_name, subject_10_septembre)\n",
    "IMAGE_DIR = Path(image_dir_10_septembre)\n",
    "\n",
    "# ====== Préparation du dataframe ======\n",
    "# (df_10_septembre doit déjà contenir x, y, cluster, filename)\n",
    "def img_to_base64(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img.thumbnail((400, 400))\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return \"data:image/png;base64,\" + base64.b64encode(buffer.getvalue()).decode()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_10_septembre[\"img_b64\"] = [img_to_base64(IMAGE_DIR / fn) for fn in df_10_septembre[\"filename\"]]\n",
    "\n",
    "# ====== Création du scatter ======\n",
    "fig = px.scatter(\n",
    "    df_10_septembre,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"cluster\",\n",
    "    hover_data=[\"filename\"],\n",
    "    custom_data=[\"filename\"],\n",
    "    title=\"Clustering visuel des images (DINOv2 + HDBSCAN)\",\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.8))\n",
    "\n",
    "# ====== Création de l'application Dash ======\n",
    "app = Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H3(\"Exploration interactive des clusters d’images (DINOv2)\"),\n",
    "    html.Div([\n",
    "        dcc.Graph(\n",
    "            id=\"scatter\",\n",
    "            figure=fig,\n",
    "            style={\"width\": \"65vw\", \"height\": \"85vh\", \"display\": \"inline-block\"},\n",
    "        ),\n",
    "        html.Div(\n",
    "            id=\"image-display\",\n",
    "            style={\"width\": \"30vw\", \"display\": \"inline-block\", \"verticalAlign\": \"top\", \"padding\": \"20px\"},\n",
    "        ),\n",
    "    ]),\n",
    "])\n",
    "\n",
    "# ====== Callback : clic sur un point → affiche l’image ======\n",
    "@app.callback(\n",
    "    Output(\"image-display\", \"children\"),\n",
    "    Input(\"scatter\", \"clickData\"),\n",
    ")\n",
    "def show_image(clickData):\n",
    "    if clickData is None:\n",
    "        return html.P(\"Clique sur un point pour afficher l'image correspondante.\")\n",
    "    \n",
    "    filename = clickData[\"points\"][0][\"customdata\"][0]\n",
    "    path = IMAGE_DIR / filename\n",
    "    if not path.exists():\n",
    "        return html.P(f\"Image introuvable : {filename}\")\n",
    "    \n",
    "    img_b64 = img_to_base64(path)\n",
    "    return html.Div([\n",
    "        html.H4(filename),\n",
    "        html.Img(src=img_b64, style={\"maxWidth\": \"100%\", \"border\": \"2px solid #444\"}),\n",
    "    ])\n",
    "\n",
    "\n",
    "# ====== Lancer le serveur ======\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, port=8050)\n",
    "\n",
    "print(\"Pour ouvrir la visualisation sur un navigateur: http://127.0.0.1:8050\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a2260",
   "metadata": {},
   "source": [
    "## Recognition france-palestine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_rfp_dino = np.vstack(df_reconnaissance_france_palestine[\"embedding\"].values)\n",
    "print(\"Shape:\", X_rfp_dino.shape)  # (nb_images, 1280)\n",
    "X_rfp_reduced = umap.UMAP(n_components=2, random_state=42, n_neighbors=20, metric='cosine', min_dist=0, spread=1, n_jobs=-1).fit_transform(X_rfp_dino)\n",
    "df_reconnaissance_france_palestine[\"x\"] = X_rfp_reduced[:,0]\n",
    "df_reconnaissance_france_palestine[\"y\"] = X_rfp_reduced[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753fbc44",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bccde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_performance(X_rfp_reduced, df_reconnaissance_france_palestine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf905435",
   "metadata": {},
   "source": [
    "The best compremise was when min cluster size is 44 (noise xx%,a good silhouette value of xx and the higher persistance xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbdc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=177, min_samples=5, metric=\"euclidean\")\n",
    "labels = clusterer.fit_predict(X_rfp_reduced)\n",
    "df_reconnaissance_france_palestine[\"cluster\"] = (labels).astype(str)\n",
    "print(f\"Cluster: {len(df_reconnaissance_france_palestine.cluster.unique())}, Bruit: {len(df_reconnaissance_france_palestine[df_reconnaissance_france_palestine.cluster == '-1'])/len(df_reconnaissance_france_palestine)*100}%, Persistance: {(clusterer.cluster_persistence_).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1961af93",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b87aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_images_name = os.path.join(path, \"img_data\")\n",
    "image_dir_reconnaissance_france_palestine = os.path.join(path_folder_images_name, subject_reconnaissance_france_palestine)\n",
    "IMAGE_DIR = Path(image_dir_reconnaissance_france_palestine)\n",
    "\n",
    "# ====== Préparation du dataframe ======\n",
    "# (df_reconnaissance_france_palestine doit déjà contenir x, y, cluster, filename)\n",
    "def img_to_base64(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img.thumbnail((400, 400))\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return \"data:image/png;base64,\" + base64.b64encode(buffer.getvalue()).decode()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_reconnaissance_france_palestine[\"img_b64\"] = [img_to_base64(IMAGE_DIR / fn) for fn in df_reconnaissance_france_palestine[\"filename\"]]\n",
    "\n",
    "# ====== Création du scatter ======\n",
    "fig = px.scatter(\n",
    "    df_reconnaissance_france_palestine,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"cluster\",\n",
    "    hover_data=[\"filename\"],\n",
    "    custom_data=[\"filename\"],\n",
    "    title=\"Clustering visuel des images (DINOv2 + HDBSCAN)\",\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.8))\n",
    "\n",
    "# ====== Création de l'application Dash ======\n",
    "app = Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H3(\"Exploration interactive des clusters d’images (DINOv2)\"),\n",
    "    html.Div([\n",
    "        dcc.Graph(\n",
    "            id=\"scatter\",\n",
    "            figure=fig,\n",
    "            style={\"width\": \"65vw\", \"height\": \"85vh\", \"display\": \"inline-block\"},\n",
    "        ),\n",
    "        html.Div(\n",
    "            id=\"image-display\",\n",
    "            style={\"width\": \"30vw\", \"display\": \"inline-block\", \"verticalAlign\": \"top\", \"padding\": \"20px\"},\n",
    "        ),\n",
    "    ]),\n",
    "])\n",
    "\n",
    "# ====== Callback : clic sur un point → affiche l’image ======\n",
    "@app.callback(\n",
    "    Output(\"image-display\", \"children\"),\n",
    "    Input(\"scatter\", \"clickData\"),\n",
    ")\n",
    "def show_image(clickData):\n",
    "    if clickData is None:\n",
    "        return html.P(\"Clique sur un point pour afficher l'image correspondante.\")\n",
    "    \n",
    "    filename = clickData[\"points\"][0][\"customdata\"][0]\n",
    "    path = IMAGE_DIR / filename\n",
    "    if not path.exists():\n",
    "        return html.P(f\"Image introuvable : {filename}\")\n",
    "    \n",
    "    img_b64 = img_to_base64(path)\n",
    "    return html.Div([\n",
    "        html.H4(filename),\n",
    "        html.Img(src=img_b64, style={\"maxWidth\": \"100%\", \"border\": \"2px solid #444\"}),\n",
    "    ])\n",
    "\n",
    "\n",
    "# ====== Lancer le serveur ======\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, port=8050)\n",
    "\n",
    "print(\"Pour ouvrir la visualisation sur un navigateur: http://127.0.0.1:8050\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e3696",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This visual clusterings experiment demonstrates how **DINO embeddings** can effectively group images according to **visual similarity** rather than textual or semantic content.  \n",
    "\n",
    "In the example above:\n",
    "- Images sharing similar **visual patterns**—such as **maps of Palestine** or **portraits of political figures**—naturally cluster together in the 2D projection.  \n",
    "- Each color represents a distinct **visual cluster**, learned purely from pixel-level and structural features.  \n",
    "\n",
    "This confirms that **self-supervised visual representations** (like DINO) are powerful tools for exploring large collections of images, detecting **recurring motifs**, and revealing **visual narratives** that may not be captured by text-based models such as CLIP.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
