{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902460e8",
   "metadata": {},
   "source": [
    "# Notebook Objective\n",
    "\n",
    "The primary objective of this notebook is to perform semantic clustering of social-media images.\n",
    "For this reason, we will exclusively import and use CLIP embeddings, which are specifically designed to capture semantic patterns (image's meaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ceb89b",
   "metadata": {},
   "source": [
    "# Test Datasets - recap\n",
    "\n",
    "We will test the search engine on two datasets:\n",
    "\n",
    "1. **Recognition of the Palestinian State (08/13 – 08/29)**  \n",
    "   This dataset was collected following France’s recognition of the State of Palestine.  \n",
    "   It includes both **texts and their associated images**, totaling **5,055 images**.\n",
    "\n",
    "2. **September 10 Demonstrations (08/20 – ~10/10)**  \n",
    "   This dataset was collected during the period surrounding the **September 10 protest in France**.  \n",
    "   It also contains **texts and their related images**, totaling **41,942 images**.\n",
    "\n",
    "Only the **image URLs** are stored — the images themselves will be **downloaded dynamically** during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d70f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "from pathlib import Path \n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import warnings \n",
    "import logging  \n",
    "import os\n",
    "\n",
    "\n",
    "from dash import Dash, dcc, html, Input, Output\n",
    "import plotly.express as px\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"transformer\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"PIL\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()        \n",
    "path = os.getenv(\"path_folder\")\n",
    "\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d01765c",
   "metadata": {},
   "source": [
    "## Import embeddings\n",
    "\n",
    "- **`path_folder_images_embeddings`** → the **directory path** where the embeddings are saved as parquet file (to be adapted as needed).\n",
    "- **`subject`** → defines the dataset used, either `\"10_septembre\"` or `\"reconnaissance_france_palestine\"` (to be adapted).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c7951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_images_embeddings = os.path.join(path, \"img_embeddings\")\n",
    "subject_10_septembre = '10_septembre' \n",
    "subject_reconnaissance_france_palestine = 'reconnaissance_france_palestine'\n",
    "\n",
    "path_10_septembre = f\"{path_folder_images_embeddings}/{subject_10_septembre}_CLIP.parquet\"\n",
    "path_reconnaissance_france_palestine = f\"{path_folder_images_embeddings}/{subject_reconnaissance_france_palestine}_CLIP.parquet\"\n",
    "\n",
    "# Upload data\n",
    "df_10_septembre = pd.read_parquet(path_10_septembre)\n",
    "df_reconnaissance_france_palestine = pd.read_parquet(path_reconnaissance_france_palestine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8e5fc1",
   "metadata": {},
   "source": [
    "## 10 septembre  \n",
    "We begin with the **“10 septembre”** dataset before moving on to the second one.  \n",
    "This approach helps avoid overlapping or duplicated code blocks that perform the same operations with only minor name changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7762cd",
   "metadata": {},
   "source": [
    "### Dimensionality reduction with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e2c509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (39839, 1024)\n",
      "CPU times: total: 3min 14s\n",
      "Wall time: 52.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_10_septembre = np.vstack(df_10_septembre[\"embedding\"].values)\n",
    "print(\"Shape:\", X_10_septembre.shape)  # (nb_images, 1280)\n",
    "X_10_septembre_reduced = umap.UMAP(n_components=2, random_state=42, n_neighbors=20, metric='cosine', min_dist=0, spread=1, n_jobs=-1).fit_transform(X_10_septembre)\n",
    "df_10_septembre[\"x\"] = X_10_septembre_reduced[:,0]\n",
    "df_10_septembre[\"y\"] = X_10_septembre_reduced[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e229c97b",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "493721a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=2\n",
      "Clusters: 1945, Bruit: 31.76%, Persistance: 0.011, Silhouette: 0.578\n",
      "i=7\n",
      "Clusters: 1145, Bruit: 28.88%, Persistance: 0.022, Silhouette: 0.596\n",
      "i=12\n",
      "Clusters: 729, Bruit: 29.05%, Persistance: 0.025, Silhouette: 0.593\n",
      "i=17\n",
      "Clusters: 538, Bruit: 30.18%, Persistance: 0.024, Silhouette: 0.598\n",
      "i=22\n",
      "Clusters: 419, Bruit: 28.72%, Persistance: 0.025, Silhouette: 0.540\n",
      "i=27\n",
      "Clusters: 334, Bruit: 29.44%, Persistance: 0.022, Silhouette: 0.529\n",
      "i=32\n",
      "Clusters: 293, Bruit: 31.53%, Persistance: 0.021, Silhouette: 0.560\n",
      "i=37\n",
      "Clusters: 243, Bruit: 31.95%, Persistance: 0.021, Silhouette: 0.538\n",
      "i=42\n",
      "Clusters: 214, Bruit: 33.27%, Persistance: 0.020, Silhouette: 0.527\n",
      "i=47\n",
      "Clusters: 198, Bruit: 34.34%, Persistance: 0.020, Silhouette: 0.520\n",
      "i=52\n",
      "Clusters: 173, Bruit: 33.46%, Persistance: 0.022, Silhouette: 0.511\n",
      "i=57\n",
      "Clusters: 140, Bruit: 29.66%, Persistance: 0.026, Silhouette: 0.457\n",
      "i=62\n",
      "Clusters: 130, Bruit: 29.57%, Persistance: 0.018, Silhouette: 0.454\n",
      "i=67\n",
      "Clusters: 125, Bruit: 30.20%, Persistance: 0.019, Silhouette: 0.457\n",
      "i=72\n",
      "Clusters: 114, Bruit: 30.18%, Persistance: 0.021, Silhouette: 0.476\n",
      "i=77\n",
      "Clusters: 103, Bruit: 31.06%, Persistance: 0.024, Silhouette: 0.475\n",
      "i=82\n",
      "Clusters: 97, Bruit: 30.00%, Persistance: 0.026, Silhouette: 0.464\n",
      "i=87\n",
      "Clusters: 91, Bruit: 29.87%, Persistance: 0.029, Silhouette: 0.482\n",
      "i=92\n",
      "Clusters: 89, Bruit: 29.84%, Persistance: 0.033, Silhouette: 0.473\n",
      "i=97\n",
      "Clusters: 86, Bruit: 30.55%, Persistance: 0.035, Silhouette: 0.474\n",
      "i=102\n",
      "Clusters: 84, Bruit: 30.75%, Persistance: 0.034, Silhouette: 0.474\n",
      "i=107\n",
      "Clusters: 80, Bruit: 31.81%, Persistance: 0.035, Silhouette: 0.474\n",
      "i=112\n",
      "Clusters: 75, Bruit: 31.47%, Persistance: 0.031, Silhouette: 0.470\n",
      "i=117\n",
      "Clusters: 71, Bruit: 32.23%, Persistance: 0.031, Silhouette: 0.478\n",
      "i=122\n",
      "Clusters: 66, Bruit: 32.77%, Persistance: 0.033, Silhouette: 0.474\n",
      "i=127\n",
      "Clusters: 63, Bruit: 33.40%, Persistance: 0.034, Silhouette: 0.488\n",
      "i=132\n",
      "Clusters: 61, Bruit: 33.59%, Persistance: 0.036, Silhouette: 0.502\n",
      "i=137\n",
      "Clusters: 59, Bruit: 34.26%, Persistance: 0.038, Silhouette: 0.506\n",
      "i=142\n",
      "Clusters: 56, Bruit: 35.31%, Persistance: 0.043, Silhouette: 0.510\n",
      "i=147\n",
      "Clusters: 56, Bruit: 35.31%, Persistance: 0.042, Silhouette: 0.510\n",
      "i=152\n",
      "Clusters: 53, Bruit: 36.06%, Persistance: 0.050, Silhouette: 0.513\n",
      "i=157\n",
      "Clusters: 51, Bruit: 36.38%, Persistance: 0.054, Silhouette: 0.515\n",
      "i=162\n",
      "Clusters: 50, Bruit: 35.33%, Persistance: 0.057, Silhouette: 0.504\n",
      "i=167\n",
      "Clusters: 52, Bruit: 36.91%, Persistance: 0.064, Silhouette: 0.523\n",
      "i=172\n",
      "Clusters: 51, Bruit: 37.33%, Persistance: 0.058, Silhouette: 0.523\n",
      "i=177\n",
      "Clusters: 51, Bruit: 37.33%, Persistance: 0.053, Silhouette: 0.523\n",
      "i=182\n",
      "Clusters: 50, Bruit: 37.77%, Persistance: 0.052, Silhouette: 0.523\n",
      "i=187\n",
      "Clusters: 47, Bruit: 36.66%, Persistance: 0.056, Silhouette: 0.503\n",
      "i=192\n",
      "Clusters: 44, Bruit: 37.53%, Persistance: 0.059, Silhouette: 0.499\n",
      "i=197\n",
      "Clusters: 43, Bruit: 38.01%, Persistance: 0.064, Silhouette: 0.499\n"
     ]
    }
   ],
   "source": [
    "test_performance(X_10_septembre_reduced, df_10_septembre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817857d",
   "metadata": {},
   "source": [
    "The best compremise was when min cluster size is 167 (noise 36.91%,a good silhouette value of 0.523 and the higher persistance 0.064)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd32d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 53, Bruit: 36.906046838525064%, Persistance: 0.0635862653560839\n"
     ]
    }
   ],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=167, min_samples=5, metric=\"euclidean\")\n",
    "labels = clusterer.fit_predict(X_10_septembre_reduced)\n",
    "df_10_septembre[\"cluster\"] = (labels).astype(str)\n",
    "print(f\"Cluster: {len(df_10_septembre.cluster.unique())}, Bruit: {len(df_10_septembre[df_10_septembre.cluster == '-1'])/len(df_10_septembre)*100}%, Persistance: {(clusterer.cluster_persistence_).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7fa01f",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107920f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8053/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x15e11e87850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour ouvrir la visualisation sur un navigateur: http://127.0.0.1:8053\n"
     ]
    }
   ],
   "source": [
    "path_folder_images_name = os.path.join(path, \"img_data\")\n",
    "image_dir_10_septembre = os.path.join(path_folder_images_name, subject_10_septembre)\n",
    "IMAGE_DIR = Path(image_dir_10_septembre)\n",
    "\n",
    "# ====== Préparation du dataframe ======\n",
    "# (df_10_septembre doit déjà contenir x, y, cluster, filename)\n",
    "def img_to_base64(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img.thumbnail((400, 400))\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return \"data:image/png;base64,\" + base64.b64encode(buffer.getvalue()).decode()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_10_septembre[\"img_b64\"] = [img_to_base64(IMAGE_DIR / fn) for fn in df_10_septembre[\"filename\"]]\n",
    "\n",
    "# ====== Création du scatter ======\n",
    "fig = px.scatter(\n",
    "    df_10_septembre,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"cluster\",\n",
    "    hover_data=[\"filename\"],\n",
    "    custom_data=[\"filename\"],\n",
    "    title=\"Clustering visuel des images (CLIP + HDBSCAN)\",\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.8))\n",
    "\n",
    "# ====== Création de l'application Dash ======\n",
    "app = Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H3(\"Exploration interactive des clusters d’images (CLIP)\"),\n",
    "    html.Div([\n",
    "        dcc.Graph(\n",
    "            id=\"scatter\",\n",
    "            figure=fig,\n",
    "            style={\"width\": \"65vw\", \"height\": \"85vh\", \"display\": \"inline-block\"},\n",
    "        ),\n",
    "        html.Div(\n",
    "            id=\"image-display\",\n",
    "            style={\"width\": \"30vw\", \"display\": \"inline-block\", \"verticalAlign\": \"top\", \"padding\": \"20px\"},\n",
    "        ),\n",
    "    ]),\n",
    "])\n",
    "\n",
    "# ====== Callback : clic sur un point → affiche l’image ======\n",
    "@app.callback(\n",
    "    Output(\"image-display\", \"children\"),\n",
    "    Input(\"scatter\", \"clickData\"),\n",
    ")\n",
    "def show_image(clickData):\n",
    "    if clickData is None:\n",
    "        return html.P(\"Clique sur un point pour afficher l'image correspondante.\")\n",
    "    \n",
    "    filename = clickData[\"points\"][0][\"customdata\"][0]\n",
    "    path = IMAGE_DIR / filename\n",
    "    if not path.exists():\n",
    "        return html.P(f\"Image introuvable : {filename}\")\n",
    "    \n",
    "    img_b64 = img_to_base64(path)\n",
    "    return html.Div([\n",
    "        html.H4(filename),\n",
    "        html.Img(src=img_b64, style={\"maxWidth\": \"100%\", \"border\": \"2px solid #444\"}),\n",
    "    ])\n",
    "\n",
    "\n",
    "# ====== Lancer le serveur ======\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, port=8053)\n",
    "\n",
    "print(\"Pour ouvrir la visualisation sur un navigateur: http://127.0.0.1:8053\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0066b4b",
   "metadata": {},
   "source": [
    "## Recognition france-palestine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf4aff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (5025, 1024)\n",
      "CPU times: total: 7 s\n",
      "Wall time: 7.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_rfp_dino = np.vstack(df_reconnaissance_france_palestine[\"embedding\"].values)\n",
    "print(\"Shape:\", X_rfp_dino.shape)  # (nb_images, 1280)\n",
    "X_rfp_reduced = umap.UMAP(n_components=2, random_state=42, n_neighbors=20, metric='cosine', min_dist=0, spread=1, n_jobs=-1).fit_transform(X_rfp_dino)\n",
    "df_reconnaissance_france_palestine[\"x\"] = X_rfp_reduced[:,0]\n",
    "df_reconnaissance_france_palestine[\"y\"] = X_rfp_reduced[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18457d4",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07720af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=2\n",
      "Clusters: 293, Bruit: 28.02%, Persistance: 0.017, Silhouette: 0.698\n",
      "i=7\n",
      "Clusters: 199, Bruit: 23.88%, Persistance: 0.024, Silhouette: 0.653\n",
      "i=12\n",
      "Clusters: 121, Bruit: 23.20%, Persistance: 0.027, Silhouette: 0.617\n",
      "i=17\n",
      "Clusters: 87, Bruit: 24.60%, Persistance: 0.009, Silhouette: 0.604\n",
      "i=22\n",
      "Clusters: 67, Bruit: 29.03%, Persistance: 0.012, Silhouette: 0.588\n",
      "i=27\n",
      "Clusters: 49, Bruit: 25.85%, Persistance: 0.018, Silhouette: 0.539\n",
      "i=32\n",
      "Clusters: 45, Bruit: 25.01%, Persistance: 0.056, Silhouette: 0.537\n",
      "i=37\n",
      "Clusters: 39, Bruit: 23.66%, Persistance: 0.056, Silhouette: 0.498\n",
      "i=42\n",
      "Clusters: 30, Bruit: 27.60%, Persistance: 0.082, Silhouette: 0.480\n",
      "i=47\n",
      "Clusters: 28, Bruit: 29.39%, Persistance: 0.083, Silhouette: 0.497\n",
      "i=52\n",
      "Clusters: 26, Bruit: 31.28%, Persistance: 0.082, Silhouette: 0.511\n",
      "i=57\n",
      "Clusters: 23, Bruit: 34.51%, Persistance: 0.077, Silhouette: 0.533\n",
      "i=62\n",
      "Clusters: 20, Bruit: 38.09%, Persistance: 0.113, Silhouette: 0.536\n",
      "i=67\n",
      "Clusters: 20, Bruit: 38.09%, Persistance: 0.091, Silhouette: 0.536\n",
      "i=72\n",
      "Clusters: 18, Bruit: 40.78%, Persistance: 0.084, Silhouette: 0.540\n",
      "i=77\n",
      "Clusters: 17, Bruit: 42.25%, Persistance: 0.077, Silhouette: 0.528\n",
      "i=82\n",
      "Clusters: 17, Bruit: 42.25%, Persistance: 0.079, Silhouette: 0.528\n",
      "i=87\n",
      "Clusters: 2, Bruit: 0.00%, Persistance: 0.488, Silhouette: 0.792\n",
      "i=92\n",
      "Clusters: 2, Bruit: 0.00%, Persistance: 0.522, Silhouette: 0.792\n",
      "i=97\n",
      "Clusters: 12, Bruit: 42.35%, Persistance: 0.191, Silhouette: 0.457\n",
      "i=102\n",
      "Clusters: 11, Bruit: 44.36%, Persistance: 0.195, Silhouette: 0.486\n",
      "i=107\n",
      "Clusters: 11, Bruit: 44.36%, Persistance: 0.198, Silhouette: 0.486\n",
      "i=112\n",
      "Clusters: 11, Bruit: 44.36%, Persistance: 0.182, Silhouette: 0.486\n",
      "i=117\n",
      "Clusters: 11, Bruit: 44.36%, Persistance: 0.160, Silhouette: 0.486\n",
      "i=122\n",
      "Clusters: 10, Bruit: 44.30%, Persistance: 0.151, Silhouette: 0.511\n",
      "i=127\n",
      "Clusters: 8, Bruit: 49.27%, Persistance: 0.168, Silhouette: 0.513\n",
      "i=132\n",
      "Clusters: 8, Bruit: 49.27%, Persistance: 0.160, Silhouette: 0.513\n",
      "i=137\n",
      "Clusters: 8, Bruit: 49.27%, Persistance: 0.145, Silhouette: 0.513\n",
      "i=142\n",
      "Clusters: 7, Bruit: 52.08%, Persistance: 0.154, Silhouette: 0.554\n",
      "i=147\n",
      "Clusters: 7, Bruit: 52.08%, Persistance: 0.146, Silhouette: 0.554\n",
      "i=152\n",
      "Clusters: 6, Bruit: 55.06%, Persistance: 0.148, Silhouette: 0.549\n",
      "i=157\n",
      "Clusters: 6, Bruit: 55.06%, Persistance: 0.143, Silhouette: 0.549\n",
      "i=162\n",
      "Clusters: 5, Bruit: 58.21%, Persistance: 0.170, Silhouette: 0.551\n",
      "i=167\n",
      "Clusters: 5, Bruit: 58.21%, Persistance: 0.170, Silhouette: 0.551\n",
      "i=172\n",
      "Clusters: 5, Bruit: 58.21%, Persistance: 0.167, Silhouette: 0.551\n",
      "i=177\n",
      "Clusters: 5, Bruit: 58.21%, Persistance: 0.167, Silhouette: 0.551\n",
      "i=182\n",
      "Clusters: 5, Bruit: 58.21%, Persistance: 0.167, Silhouette: 0.551\n",
      "i=187\n",
      "Clusters: 5, Bruit: 58.21%, Persistance: 0.167, Silhouette: 0.551\n",
      "i=192\n",
      "Clusters: 5, Bruit: 58.21%, Persistance: 0.165, Silhouette: 0.551\n",
      "i=197\n",
      "Clusters: 5, Bruit: 58.21%, Persistance: 0.165, Silhouette: 0.551\n"
     ]
    }
   ],
   "source": [
    "test_performance(X_rfp_reduced, df_reconnaissance_france_palestine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c49f5f",
   "metadata": {},
   "source": [
    "The best compremise was when min cluster size is 37 (noise 23.66%,a good silhouette value of 0.498 and a persistance of 0.056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "831feb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 40, Bruit: 23.66169154228856%, Persistance: 0.05601262146102711\n"
     ]
    }
   ],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=37, min_samples=5, metric=\"euclidean\")\n",
    "labels = clusterer.fit_predict(X_rfp_reduced)\n",
    "df_reconnaissance_france_palestine[\"cluster\"] = (labels).astype(str)\n",
    "print(f\"Cluster: {len(df_reconnaissance_france_palestine.cluster.unique())}, Bruit: {len(df_reconnaissance_france_palestine[df_reconnaissance_france_palestine.cluster == '-1'])/len(df_reconnaissance_france_palestine)*100}%, Persistance: {(clusterer.cluster_persistence_).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370f7a3b",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839c54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8054/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x15dc42b0f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour ouvrir la visualisation sur un navigateur: http://127.0.0.1:8054\n"
     ]
    }
   ],
   "source": [
    "path_folder_images_name = os.path.join(path, \"img_data\")\n",
    "image_dir_reconnaissance_france_palestine = os.path.join(path_folder_images_name, subject_reconnaissance_france_palestine)\n",
    "IMAGE_DIR = Path(image_dir_reconnaissance_france_palestine)\n",
    "\n",
    "# ====== Préparation du dataframe ======\n",
    "# (df_reconnaissance_france_palestine doit déjà contenir x, y, cluster, filename)\n",
    "def img_to_base64(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img.thumbnail((400, 400))\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return \"data:image/png;base64,\" + base64.b64encode(buffer.getvalue()).decode()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_reconnaissance_france_palestine[\"img_b64\"] = [img_to_base64(IMAGE_DIR / fn) for fn in df_reconnaissance_france_palestine[\"filename\"]]\n",
    "\n",
    "# ====== Création du scatter ======\n",
    "fig = px.scatter(\n",
    "    df_reconnaissance_france_palestine,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"cluster\",\n",
    "    hover_data=[\"filename\"],\n",
    "    custom_data=[\"filename\"],\n",
    "    title=\"Clustering visuel des images (CLIP + HDBSCAN)\",\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.8))\n",
    "\n",
    "# ====== Création de l'application Dash ======\n",
    "app = Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H3(\"Exploration interactive des clusters d’images (CLIP)\"),\n",
    "    html.Div([\n",
    "        dcc.Graph(\n",
    "            id=\"scatter\",\n",
    "            figure=fig,\n",
    "            style={\"width\": \"65vw\", \"height\": \"85vh\", \"display\": \"inline-block\"},\n",
    "        ),\n",
    "        html.Div(\n",
    "            id=\"image-display\",\n",
    "            style={\"width\": \"30vw\", \"display\": \"inline-block\", \"verticalAlign\": \"top\", \"padding\": \"20px\"},\n",
    "        ),\n",
    "    ]),\n",
    "])\n",
    "\n",
    "# ====== Callback : clic sur un point → affiche l’image ======\n",
    "@app.callback(\n",
    "    Output(\"image-display\", \"children\"),\n",
    "    Input(\"scatter\", \"clickData\"),\n",
    ")\n",
    "def show_image(clickData):\n",
    "    if clickData is None:\n",
    "        return html.P(\"Clique sur un point pour afficher l'image correspondante.\")\n",
    "    \n",
    "    filename = clickData[\"points\"][0][\"customdata\"][0]\n",
    "    path = IMAGE_DIR / filename\n",
    "    if not path.exists():\n",
    "        return html.P(f\"Image introuvable : {filename}\")\n",
    "    \n",
    "    img_b64 = img_to_base64(path)\n",
    "    return html.Div([\n",
    "        html.H4(filename),\n",
    "        html.Img(src=img_b64, style={\"maxWidth\": \"100%\", \"border\": \"2px solid #444\"}),\n",
    "    ])\n",
    "\n",
    "# ====== Lancer le serveur ======\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, port=8054)\n",
    "\n",
    "print(\"Pour ouvrir la visualisation sur un navigateur: http://127.0.0.1:8054\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a67e3",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This semantic clustering experiment highlights how **CLIP embeddings** organize images based on their **conceptual meaning** rather than purely visual features.  \n",
    "\n",
    "In the example above:\n",
    "- Images and texts referring to similar **themes, entities, or narratives** (e.g., *Zionism*, *political leaders*, *war scenes*) are grouped together in the embedding space.  \n",
    "- Each cluster reflects a **shared semantic context**, even when the visual appearance differs significantly.  \n",
    "\n",
    "This demonstrates the strength of **multimodal models** like CLIP, which align visual and textual information into a unified representation.  \n",
    "By mapping both images and language into the same latent space, CLIP enables **semantic grouping, retrieval, and narrative analysis** — allowing us to explore how visual content connects to the broader discourse it represents.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
